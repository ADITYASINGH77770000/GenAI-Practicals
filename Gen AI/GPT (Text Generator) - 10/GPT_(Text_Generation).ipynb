{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "56ab1cf38fb44eee871f6e8badf459b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c31b86181cec43f29bc528f6e339685a",
              "IPY_MODEL_7631cb4207ec4f12937f451a9d8e3df4",
              "IPY_MODEL_736f9163ecc14518957b362400e1f307"
            ],
            "layout": "IPY_MODEL_df0da8bfe06249fb932ae698590221e8"
          }
        },
        "c31b86181cec43f29bc528f6e339685a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_38e0bfeda2c54baba5e551ab2d77221e",
            "placeholder": "​",
            "style": "IPY_MODEL_ffa7f273141f4ca98e17d05e89178d4b",
            "value": "Map: 100%"
          }
        },
        "7631cb4207ec4f12937f451a9d8e3df4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af82dd7b80204684ab1f773ef613188e",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb8a3d37208a410c848b615dd17296a1",
            "value": 10
          }
        },
        "736f9163ecc14518957b362400e1f307": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7b1e8881cd04591b1d7de705f589bc4",
            "placeholder": "​",
            "style": "IPY_MODEL_60bf926cb0634404aef66481c985a0d1",
            "value": " 10/10 [00:00&lt;00:00, 259.40 examples/s]"
          }
        },
        "df0da8bfe06249fb932ae698590221e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38e0bfeda2c54baba5e551ab2d77221e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffa7f273141f4ca98e17d05e89178d4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af82dd7b80204684ab1f773ef613188e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb8a3d37208a410c848b615dd17296a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f7b1e8881cd04591b1d7de705f589bc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60bf926cb0634404aef66481c985a0d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 891,
          "referenced_widgets": [
            "56ab1cf38fb44eee871f6e8badf459b5",
            "c31b86181cec43f29bc528f6e339685a",
            "7631cb4207ec4f12937f451a9d8e3df4",
            "736f9163ecc14518957b362400e1f307",
            "df0da8bfe06249fb932ae698590221e8",
            "38e0bfeda2c54baba5e551ab2d77221e",
            "ffa7f273141f4ca98e17d05e89178d4b",
            "af82dd7b80204684ab1f773ef613188e",
            "cb8a3d37208a410c848b615dd17296a1",
            "f7b1e8881cd04591b1d7de705f589bc4",
            "60bf926cb0634404aef66481c985a0d1"
          ]
        },
        "id": "42eiJUTKL4M3",
        "outputId": "398039da-360b-4bb1-ec30-dc7a5f4ae35f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Loaded dataset. Examples: 10\n",
            "Applied LoRA. Trainable params (PEFT): 589824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56ab1cf38fb44eee871f6e8badf459b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized dataset sample: tensor([16594,   257,   387, 28643,   546, 40048,    13,  8407,  1657, 42123])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 00:50, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/peft/utils/save_and_load.py:300: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved fine-tuned model to /content/gpt_lora_out\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Write a short, friendly email saying you will be late to the meeting because\n",
            "\n",
            "=== Generated ===\n",
            " Write a short, friendly email saying you will be late to the meeting because there is a lot of work to be done and that the meeting will be cancelled.\n",
            "\n",
            "Please note that any delay in posting a short email will result in an immediate cancellation and may result in your email being deleted.\n",
            "\n",
            "You may contact us directly or email us at:\n",
            "\n",
            "Shenzhen Hao University\n",
            "\n",
            "Yen-Shan Hao University\n",
            "\n",
            "Tel: +886-917-879-3660\n",
            "\n",
            "Fax: +886-917-879-3660\n",
            "\n",
            "Email: hao@yenghao.edu.cn\n",
            "\n",
            "For more information on this topic, please see the Resources section of the Hao University website.\n"
          ]
        }
      ],
      "source": [
        "# Fine-tune GPT with LoRA (PEFT) - Colab-ready\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    Trainer, TrainingArguments, DataCollatorForLanguageModeling,\n",
        "    pipeline\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "\n",
        "# ---------- User settings (edit if needed) ----------\n",
        "MODEL = \"gpt2\"                # base model; change to gpt2-medium etc. if you have memory\n",
        "TRAIN_FILE = \"Training.txt\"   # upload train.txt to Colab /content/\n",
        "OUTPUT_DIR = \"/content/gpt_lora_out\"\n",
        "BATCH_SIZE = 8               # per-device batch size (lower if OOM)\n",
        "EPOCHS = 3\n",
        "LEARNING_RATE = 2e-4\n",
        "BLOCK_SIZE = 128             # max token length\n",
        "LORA_R = 16\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.1\n",
        "SEED = 42\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# check GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# 1) load dataset (plain text: one example per line)\n",
        "if not os.path.exists(TRAIN_FILE):\n",
        "    raise FileNotFoundError(f\"Training file not found: {TRAIN_FILE}. Upload train.txt to Colab /content/\")\n",
        "\n",
        "ds = load_dataset(\"text\", data_files={\"train\": TRAIN_FILE})\n",
        "print(\"Loaded dataset. Examples:\", len(ds[\"train\"]))\n",
        "\n",
        "# 2) tokenizer & base model\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
        "# ensure pad token exists\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL)\n",
        "# resize embeddings if tokenizer changed\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# 3) apply LoRA (PEFT)\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "print(\"Applied LoRA. Trainable params (PEFT):\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "\n",
        "# 4) tokenize dataset\n",
        "def tokenize_batch(examples):\n",
        "    out = tokenizer(examples[\"text\"], truncation=True, max_length=BLOCK_SIZE, padding=\"max_length\")\n",
        "    out[\"labels\"] = out[\"input_ids\"].copy()\n",
        "    return out\n",
        "\n",
        "tokenized = ds[\"train\"].map(tokenize_batch, batched=True, remove_columns=[\"text\"])\n",
        "tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "print(\"Tokenized dataset sample:\", tokenized[0][\"input_ids\"][:10])\n",
        "\n",
        "# 5) data collator and training args\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    logging_steps=50,\n",
        "    save_total_limit=3,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\",   # disable wandb by default\n",
        "    remove_unused_columns=False,\n",
        ")\n",
        "\n",
        "# 6) Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# 7) Train\n",
        "trainer.train()\n",
        "\n",
        "# 8) Save LoRA adapters + tokenizer\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "model.save_pretrained(OUTPUT_DIR)       # saves adapters + base model config required by peft\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(\"Saved fine-tuned model to\", OUTPUT_DIR)\n",
        "\n",
        "# 9) Quick generation check (loads model with adapters)\n",
        "# Load the base model and then the PEFT adapters\n",
        "# Load the tokenizer used during training from the output directory\n",
        "tokenizer_for_inference = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(MODEL)\n",
        "# Resize the base model's embeddings to match the tokenizer used during training\n",
        "base_model.resize_token_embeddings(len(tokenizer_for_inference))\n",
        "model_to_infer = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
        "model_to_infer = model_to_infer.merge_and_unload() # Merge LoRA weights for inference\n",
        "\n",
        "# Use pipeline with the combined model\n",
        "gen_device = 0 if torch.cuda.is_available() else -1\n",
        "gen_pipe = pipeline(\"text-generation\", model=model_to_infer, tokenizer=tokenizer_for_inference, device=gen_device)\n",
        "prompt = \"Write a short, friendly email saying you will be late to the meeting because\"\n",
        "print(\"Prompt:\", prompt)\n",
        "out = gen_pipe(prompt, max_length=120, do_sample=True, top_k=50, top_p=0.95, temperature=0.8, num_return_sequences=1)\n",
        "print(\"\\n=== Generated ===\\n\", out[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "OUTPUT_DIR = \"/content/gpt_lora_out\"  # where the fine-tuned model was saved\n",
        "MODEL = \"gpt2\" # specify the base model used for training\n",
        "\n",
        "# pick GPU if available\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "\n",
        "# Load the tokenizer used during training from the output directory\n",
        "tokenizer_for_inference = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Load the base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(MODEL)\n",
        "\n",
        "# Resize the base model's embeddings to match the tokenizer used during training\n",
        "base_model.resize_token_embeddings(len(tokenizer_for_inference))\n",
        "\n",
        "# Load the PEFT adapters and merge them into the base model\n",
        "model_to_infer = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
        "model_to_infer = model_to_infer.merge_and_unload() # Merge LoRA weights for inference\n",
        "\n",
        "# load pipeline with fine-tuned model\n",
        "gen = pipeline(\"text-generation\", model=model_to_infer, tokenizer=tokenizer_for_inference, device=gen_device) # Use gen_device from previous cell or define here\n",
        "\n",
        "# try some prompts\n",
        "prompts = [\n",
        "    \"Write a haiku about stars.\",\n",
        "    \"Draft a polite email apologizing for being late.\",\n",
        "    \"Explain binary search simply.\",\n",
        "    \"Translate to a friendly tone: The meeting is cancelled.\"\n",
        "]\n",
        "\n",
        "for p in prompts:\n",
        "    out = gen(p, max_length=80, do_sample=True, top_k=50, top_p=0.95, temperature=0.8)[0][\"generated_text\"]\n",
        "    print(\"\\nPrompt:\", p)\n",
        "    print(\"Generated:\", out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5KStVb3PLzV",
        "outputId": "4d7fef36-965c-4057-e1d4-007d828b91b8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: Write a haiku about stars.\n",
            "Generated: Write a haiku about stars. \"The stars are on the other side of the universe. We all have an idea about what it means to be in the other side of the universe.\"\n",
            "\n",
            "Now, I am not saying that there is not possibility to be in the other side of the universe. There is. I am saying that we are all in the other side of the universe. I am saying that there is a space-time continuum where the universe is not parallel and there is no space-time continuum. The universe is in the other side of the universe and the universe is not parallel. In other words, there is no time-space continuum and there is no time-time continuum. If you are in the other side of the universe, there is no time-time continuum or space-time continuum.\n",
            "\n",
            "For me, I was asked a great question by a friend about the time-space continuum. She said that the universe is in the other side of the universe and the universe is not parallel and there is no time-space continuum. I am also saying that there is a space-time continuum where the universe is not parallel and there is no space-time continuum. The universe is in the other side of the universe and the universe is not parallel. In other words,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: Draft a polite email apologizing for being late.\n",
            "Generated: Draft a polite email apologizing for being late.\n",
            "\n",
            "\"I never have had any issues with you,\" she wrote. \"I had a phone call from a guy who asked me if he could send me a message asking if I should send him a copy of my book, and I told him to. Then he started asking if I could help him figure out what he should do.\n",
            "\n",
            "\"I tried to explain that I was just going to send him a thank you email but they weren't going to let him do it, so I tried to work it out with him and get him to sign the thank you.\n",
            "\n",
            "\"I was told to make sure he read my book, but I didn't know how to do it, so I wrote the book for him. I was told to go back to the restaurant and get ready to send him the book. I didn't think there was any chance of him getting it and had to leave the restaurant. I had to tell him that I wanted him to read the book, so I told him that I would read it the next day, but I didn't want him to miss his opportunity to finish his book.\"\n",
            "\n",
            "She continued, \"As I mentioned, this was my first time dealing with this sort of stuff, so I had to keep my distance\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Both `max_new_tokens` (=256) and `max_length`(=80) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Prompt: Explain binary search simply.\n",
            "Generated: Explain binary search simply. If a string is not found, the search is not complete, but can be completed. If the string contains a'', a search will be performed.\n",
            "\n",
            "' : search [1, '2', '3', '4'] : search [1, '2', '3', '4'] : search [1, '2', '3', '4'] : search [1, '2', '3', '4'] : search [1, '2', '3', '4'] : search [1, '2', '3', '4'] : search [1, '2', '3', '4'] : search [1, '2', '3', '4'] : search [1, '2', '3', '4'] : search [1, '2', '3', '4'] : search [1, '2', '3', '4'] : search [1, '2', '3', '4'] : search [1, '2', '3', '4'] : search [1, '2', '3', '4'] : search [1, '2', '3', '4'] : search [1, '2', '3', '\n",
            "\n",
            "Prompt: Translate to a friendly tone: The meeting is cancelled.\n",
            "Generated: Translate to a friendly tone: The meeting is cancelled. We are in a quiet room. The police are waiting for us. We need your help. Please take the time to read what happened. If you hear anything like this, please call the police.\n",
            "\n",
            "After the meeting, the police will take us to the police station. The station is closed for the time being. We are not allowed in the area. We are waiting for our turn at the police station. We are not allowed in the area.\n",
            "\n",
            "The police told us that they were not aware of any reports of assault at the scene. The police were informed that there were two other attackers and that the two assailants took part in an assault.\n",
            "\n",
            "There are no injuries reported and no injuries reported by the police. However, we have not heard any complaints of rape or any other violence against any woman.\n",
            "\n",
            "We need your help to help us get to the police station.\n",
            "\n",
            "Sincerely,\n",
            "\n",
            "Diana,\n",
            "\n",
            "Diana,\n",
            "\n",
            "Diana,\n",
            "\n",
            "I want to make this absolutely clear: I am a woman who has been traumatised by the sexual assault of a young girl in my village. I am also a survivor of sexual violence against a young girl. I am scared about the future, my family and society\n"
          ]
        }
      ]
    }
  ]
}